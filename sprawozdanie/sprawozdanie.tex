\documentclass{classrep}
\usepackage[utf8]{inputenc}
\usepackage{color}
\usepackage{ltablex}
\usepackage{makebox}
\usepackage{amsmath}

\studycycle{Informatyka, studia dzienne, II st.}
\coursesemester{II}

\coursename{Komputerowe systemy rozpoznawania}
\courseyear{2011/2012}

\courseteacher{dr inż. Arkadiusz Tomczyk}
\coursegroup{poniedziałek, 10:15}

\author{
  \studentinfo{Michał Janiszewski}{169485} \and
  \studentinfo{Mariusz Łucka}{169493}
}

\title{Zadanie Numer 1: Ekstrakcja cech, miary podobieństwa, klasyfikacja}
\svnurl{}

\begin{document}
\maketitle

\section{Cel}
Celem zadania było stworzenie szkieletu aplikacji do klasyfikacji metodą k-NN na tyle uniwersalnego, żeby był niezależny od typu obiektów, które podlegają klasyfikacji.

Następnie dla zadanego zestawu danych tekstowych oraz zebranych przez siebie 100 krótkich tekstów należało zaimplementować dowolne dwie istniejące w literaturze metody ekstrakcji cech i dwie miary podobieństwa tekstów oraz metryki: euklidesowa, uliczna, Czebyszewa.

Kolejny krok to porównanie wyników dla poszczególnych metod oraz opcjonalnie zaproponowanie własnej metody ekstrakcji oraz miary podobieństwa, poprawiającej uzyskiwane wyniki. 

\section{Wprowadzenie}
\subsection{Algorytm k-nn}
Algorytm k najbliższych sąsiadów (lub algorytm k-nn z ang. \textit{k nearest neighbours}) jest jednym z algorytmów używanych do klasyfikacji danych. 

Algorytm zakłada istnienie zbioru uczącego zawierającego obserwacje, z których każda ma przypisany wektor zmiennych objaśniających $X$ oraz zmienną objaśnianą $Y$ (etykietę). Wtedy dla pewnej obserwacji $C$ z przypisanym wektorem zmiennych objaśniających $X$ możemy prognozować wartość zmiennej objaśnianej $Y$, czyli możemy dokonać klasyfikacji.

\paragraph{}
Algorytm polega na:

\begin{enumerate}
\item porównaniu wartości zmiennych objaśniających dla obserwacji $C$ z wartościami tych zmiennych dla każdej obserwacji w zbiorze uczącym (wg pewnej ustalonej metryki),
\item wyborze $k$ (ustalona z góry liczba) najbliższych do $C$ obserwacji ze zbioru uczącego,
\item wyborze etykiety $Y$, która występuje najczęściej wśród znalezionych $k$ obserwacji jako etykiety dla obserwacji $C$. 
\end{enumerate}

\subsection{Ekstrakcja cech tekstu}
Celem ekstrakcji cech tekstu jest reprezentacja tekstu w postaci wektorów cech, które następnie można porównać ze sobą. W przypadku tekstu za jego cechy można przyjąć występujące w nim słowa. Badanie wszystkich słów występujących w zbiorze dokumentów byłoby zbyt kosztowne, dlatego należy wybrać cześć z nich (słowa kluczowe), najlepiej charakteryzujących poszczególne dokumenty. Następnie dla każdego dokumentu należy utworzyć wektor wag liczbowych poszczególnych słów kluczowych.

\paragraph{}
Jedna z metod ograniczenia zbioru rozważanych słów polega na zliczeniu ilość wystąpień poszczególnych słów w całym zbiorze dokumentów oraz odrzuceniu tych które występują najczęściej i najrzadziej (odpowiednie progi należy dobrać doświadczalnie). W ten sposób usuwamy słowa niezwiązane z konkretną klasą dokumentów - słowa występujące tylko w pojedynczych dokumentach lub występujące w prawie każdym dokumencie jak na przykład spójniki, zaimki itd.

Do wyboru słów kluczowych wykorzystaliśmy dwa algorytmy: \textit{gęstości informacji}\footnote{W cytowanej literaturze opisywana metoda nie posiada nazwy.} oraz \textit{mutual information}\footnote{Ze względu na nieznajomość polskiego tłumaczenia tej nazwy, będziemy używać jej oryginalnego określenia. Gdybyśmy mieli nazwę tę przetłumaczyć, brzmiałaby ona następująco: ,,metoda informacji wspólnej''.}

\subsubsection{Gęstość informacji}
Dla danego zbioru $A_p$ dokumentów $X_i$ składających się ze słow $x_j$ poszukujemy takich słów, dla których parametr $f^k_l$, oznaczający liczbę wystąpień słowa $x_l$ w dokumencie $X_k$, spełnia następujące cechy:
\begin{equation}
 \exists_{A_1, A_2} A_1 \cup A_2 = A, A_1 \cap A_2 = \emptyset \mbox{ oraz } \forall_{X_i \in A_1} \forall_{X_j \in A_2} f^i_x \gg f^j_x
\end{equation}

Oznacza to, że poszukujemy słów, które występują głównie w ograniczonym zbiorze dokumentów i występują w nim dostatecznie często.

Określmy funkcję podobieństwa $P$ następująco:

\begin{equation}
 P(X_i, X_j) = \frac {\mu(X_i \cap X_j)} {\mu(X_i \cup X_j)}
\end{equation}

Po wyznaczeniu wszystkich wartości funkcji $P$ należy je zsumować z pominięciem głównej przekątnej uzyskanej macierzy:
\begin{equation}
 G = \sum _{i, j = 1; i \neq j}^{m} P(X_i, X_j)
\end{equation}
Otrzymaną w ten sposób wartość $G$ nazywać będziemy \textit{gęstością zbioru dokumentów}. Przez wartość $G_x$ oznaczymy gęstość dokumentów po usunięciu z nich słowa $x$.

\textit{Wartością dyskryminacyjną słowa $x$} będziemy nazywać wielkość $g(x)$ określoną w następujący sposób:
\begin{equation}
 g(x) = G_x - G
\end{equation}

Słowo $x$ zostanie wyznaczone jako możliwe słowo kluczowe (będzie \textit{dobrym dyskryminatorem}) jeśli będzie zwiększać gęstość zbioru dokumentów $A$ ($g(x) > 0$). Słowo $x$ zostanie odrzucone, jeśli jego wartość dyskryminacyjna będzie ujemna, ponieważ jego usunięcie zmniejsza gęstość zbioru dokumentów $A$.

  
Do wyznaczenia wag poszczególnych słów kluczowych w tesktach można wykorzystać algorytm 
\textbf{TF-IDF}.

\paragraph{}
Wartość TF-IDF oblicza się ze wzoru:

$$ tf\mbox{-}idf)_{i,j} = tf_{i,j} \times  idf_{i}$$

Gdzie $tf_{i, j}$ to tzw. "term frequency", wyrażana wzorem:

$$ tf_{i,j} = \frac{n_{i,j}}{\sum_k n_{k,j}}$$

Gdzie $n_{i,j}$ jest liczbą wystąpień słowa $t_{i}$ w dokumencie $d_{j}$, a mianownik jest sumą liczby wystąpień wszystkich termów w dokumencie $d_{j}$.


$idf_{i}$ to "inverse document frequency", wyrażana wzorem:

$$idf_{i} =  \log \frac{|D|}{|\{d: t_{i} \in d\}}$$

Gdzie:
$|D|$ - liczba dokumentów w zbiorze\\
$|\{d : t_{i} \in d\}|$ - liczba dokumentów zawierających przynajmniej jedno wystąpienie danego słowa.

\paragraph{}
Wartość TF-IDF jest największa dla najlepszych słów kluczowych, czyli takich, które najlepiej pozwalają określić klasy dokumentów. Takie słowo występuje często w danym dokumencie, a jednocześnie pojawia się niewiele razy w innych dokumentach.


\subsection{Metryki odległości wektorów liczbowych}
W tym zadaniu należało zastosować następujące metryki wektorów liczbowych, reprezentujących wydobyte cechy tekstu:

\begin{enumerate}
\item Metryka euklidesowa $$d(x,y)=\sqrt{\sum_{i=1}^{n}{(x_i-y_i)^2} } $$
\item Metryka uliczna (Manhattan) $$d(x,y)=\sum_{i=1}^{n}{|x_i-y_i|} $$
\item Metryka Czebyszewa $$d(x,y)=\max_{i=1:n}(|x_i-y_i|) $$
\end{enumerate}
Metryki te są miarami odległości wektorów $x$ i $y$,. zatem im mniejsze wartości otrzymamy tym wektory leżą bliżej siebie.

\subsection{Miary podobieństwa tekstów}
W zadania zostały wykorzystane następujące dwie miary podobieństwa tekstów:

\begin{enumerate}
\item Miara Jaccarda: $$J(A,B)=\frac{\mu(A \cap B)}{\mu(A \cup B)}$$
gdzie $A$ i $B$ to zbiory słów w dwóch dokumentach a $\mu$ oznacza liczność zbioru.

\item Metoda n-gramów.\\
Określa podobieństwo łańcuchów tekstowych $s_1$ i $s_2$ w oparciu o ilość wspólnych podciągów n-elementowych:
$$sim_n(s_1, s_2)=\frac{1}{N-n+1}\sum_{i=1}^{N-n+1}h(i)$$
gdzie:\\
 $s_1$ i $s_2$ - badane łańcuchy tekstowe,\\
 $N$ - ilość podciągów n-elementowych w $s_1$,\\
 $n$ - długość podciągu,\\
 $h(i)=1$ - jeżeli i-ty podciąg z $s_1$ występuje w $s_2$,\\
 $h(i)=0$ - jeżeli i-ty podciąg z $s_1$ nie występuje w $s_2$.
\end{enumerate}
Dla ciągów identycznych miary podobieństwa przyjmują wartość 1, a dla zupełnie różnych wartość 0.


\section{Opis implementacji}
Aplikacja została podzielona na cztery główne moduły:
\begin{itemize}
 \item \verb|sgmlExtractor| \ppauza prosta aplikacja służąca do ekstrakcji danych z plików formatu \verb|SGML| na prostszy w obsłudze format,
 \item \verb|keywordSelector| \ppauza aplikacja służąca do wyboru słów kluczowych dla zadanego tekstu,
 \item \verb|stemmer| \ppauza aplikacja dokonująca rdzeniownie tekstu,
 \item \verb|classifier| \ppauza główna aplikacja zadania, która dokonuje klasyfikacji danych wejściowych.
\end{itemize}
Z wyjątkiem aplikacji \verb|stemmer|, wszystkie opisane powyżej aplikacje wykonane zotały przy użyciu frameworka \verb|Qt|, w języku \verb|C++|.

\subsection{sgmlExtractor}
Aplikacja ta ekstrahuje dane z pliku o strukturze \verb|SGML| do pliku o następującym formacie:
\begin{verbatim}
 etykieta1
 tekst1
 etykieta2
 tekst2
\end{verbatim}
Pozwala to na szybkie wczytanie danych, a także na ich łatwą interpretację przez człowieka \ppauza dane można edytować bez większych obaw o ,,zepsucie'' formatu pliku.

Aplikacja ta zawiera klasę \verb|SgmlReader|, która w metodzie \verb|readDirectory| przeszukuje żądany katalog w poszukiwaniu plików w formacie \verb|SGML|, a jeśli takie występują, to sprawdza zawarte w nich informacje pod kątem ustawionych wcześniej cech.

Zapis do nowego formatu wykonuje klasa \verb|ArticleWriter|.

\subsection{keywordSelector}
Aplikacja implementuje dwie metody wyboru słów kluczowych: \textit{gęstości informacji} oraz \textit{mutual information}, implementowane odpowiednio przez klasy   \verb|DiscriminatingExtractor| oraz \verb|MIExtractor|.

Obie metody odziedziczają interfejs \verb|KeywordExtractorInterface|, dzięki czemu możliwy jest dogodny wybór sposobu ekstrakcji słów kluczowych.

\subsection{stemmer}
{\color{red} OPPISAĆ STEMMER w kilku slowach}

\subsection{classifier}
Aplikacja ta może działać w trzech trybach: klasyfikowania za pomocą metody $k$-NN, badania podobieństwa tekstów oraz ekstrakcji cech.

\subsubsection{Metoda $k$-NN}
\label{sec:impl_knn}
W trybie tym, zaraz po utworzeniu strumienia wyjściowego, sterowanie przekazywane jest do klasy \verb|Knn|, która inicjuje wybraną metrykę korzystając z fabryki \verb|MetricFactory|, a następnie za jej pomocą ładuje dane (które mogą być specyficzne dla żądanej metryki), po czym tworzy wątki \verb|KnnThread|, które dokonują obliczeń i wyznaczają odległości pomiędzy poszczególnymi obiektami zbiorów testowego i treningowego. Klasa \verb|Knn| konsoliduje wyniki i zapisuje je.

\subsubsection{Badanie podobieństwa tekstów}
Po wczytaniu artykułów za pomocą klasy \verb|ArticleLoader| następuje wyznaczenie wymaganych zbiorów słów, po czym podobnie jak w sekcji \ref{sec:impl_knn} sterowanie przekazywane jest do klasy \verb|Knn| z tą różnicą, że praca w tym przypadku wykonywana jest przez wątki klasy \verb|KnnSimiliarityThread|.

\subsubsection{Ekstrakcja cech}
Po wczytaniu artykułów za pomocą klasy \verb|ArticleLoader| wybierane są słowa, względem których liczone mają być wagi. Po zainicjowaniu wszystkich potrzebnych danych klasa \verb|Tfidf| wykonuje algorytm o tej samej nazwie.

\section{Materiały i metody}
Badania zostały przeprowadzone na dwóch zbiorach dokumentów tekstowych:\\
\begin{enumerate}
\item Zbiór dokumentów Reuters dostępny pod adresem:\\ http://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection\\\\
Dla tego zbioru testy były przeprowadzone w dwóch kategoriach:

\begin{itemize}
\item \textit{places} (etykiety: \textit{west-germany}, \textit{usa}, \textit{uk}, \textit{france}, \textit{canda}, \textit{japan}),
\item \textit{topics} (etykiety: \textit{coffee}, \textit{gold}, \textit{ship}, \textit{sugar}).\\
\end{itemize}
Wykorzystane zostały tylko artykuły, które w danej kategorii posiadają dokładnie jedną z wymienionych etykiet.\\


\item Zbiór wybranych na potrzeby tego zadania 100 krótkich opisów potraw ze strony http://allrecipes.com, z etykietami: \textit{cake} i \textit{pasta} (50 opisów dla każdej kategorii).

\end{enumerate}

\paragraph{}
We wszystkich badaniach zbiór danych został podzielony na 60\% danych uczących i 40\% danych testowych.
\paragraph{}
W każdym zbiorze tekstów zostały wyodrębnione wektory cech dla poszczególnych dokumentów metodami opisanymi w rozdziale 2, a następnie dokonano klasyfikacji za pomocą algorytmu k-NN dla $k$ z zakresu 1-100. Do badania odległości wektorów wykorzystano opisane wcześniej metryki: eklidesową, uliczną i Czebyszewa.\\
Również dla każdego zestawu tekstów policzono wartości podobieństwa ze tekstów zbioru treningowego do tekstów ze zbioru uczącego i dokonano klasyfikacji za pomocą k-NN.

Dodatkowo w celu poprawy otrzymywanych wyników została zastosowana zmodyfikowana przez nas metoda Jaccarda do określania podobieństwa tekstów: $$sim(d_1, d_2) = \sqrt{J(A ,B)*J(C, D)}$$\\
gdzie:\\
$A$ i $B$ to zbiory słów w dokumentach $d_1$ i $d_2$,\\
$C$ i $D$ to zbiory n-gramów 4-elementowych w dokumentach $d_1$ i $d_2$.

\paragraph{} 
{\color{red}OPISAĆ ULEPSZONĄ METODĘ EKSTRAKCJI}

\section{Wyniki}
Poniższe tabele przedstawiają otrzymane podczas badań wyniki:

%tabela 1

\begingroup
{\scriptsize  
\setlength{\LTleft}{-20cm plus -1fill}
\setlength{\LTright}{\LTleft}

\begin{longtable}{|p{1cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{0.7cm}|p{1.1cm}|}
\caption{ Procent poprawnych wyników dla pierwszej metody ekstrakcji i metryki euklidesowej.}\\ 
\hline

Zbiór
dokumentów

 &\multicolumn{15}{c|}{Parametr k}\\
\cline{2-16}
& 1
& 2
& 3
& 4
& 5
& 6
& 7
& 8
& 9
& 10
& 20
& 30
& 60
& 100
& Najlepszy wynik
\\ \hline\hline
Reuters
- places
& 87.86\%	%1
& 85.03\%	%2
& 89.56\%	%3
& 88.91\%	%4
& 89.87\%	%5
& 90.04\%	%6
& 90.41\%	%7
& 90.17\%	%8
& 90.50\%	%9
& 90.17\%	%10
& 89.36\%	%20
& 88.64\%	%30
& 88.05\%	%60
& 86.86\%	%100
& 90.17\% (k=8,10)
\\ \hline
Reuters
- topics
&
&
&
&
&
&
&
&
&
&
&
&
&
&
&
\\ \hline
Przepisy
- kulinarne 
&
&
&
&
&
&
&
&
&
&
&
&
&
&
&
\\ \hline
\end{longtable}
}
\endgroup

{\color{blue}
W tej sekcji należy zaprezentować, dla każdego przeprowadzonego eksperymentu,
kompletny zestaw wyników w postaci tabel, wykresów itp. Powinny być one tak
ponazywane, aby było wiadomo, do czego się odnoszą. Wszystkie tabele i wykresy
należy oczywiście opisać (opisać co jest na osiach, w kolumnach itd.) stosując
się do przyjętych wcześniej oznaczeń. Nie należy tu komentować i interpretować
wyników, gdyż miejsce na to jest w kolejnej sekcji. Tu również dobrze jest
wprowadzić oznaczenia (tabel, wykresów) aby móc się do nich odwoływać
poniżej.}

\section{Dyskusja}
{\color{blue}
Sekcja ta powinna zawierać dokładną interpretację uzyskanych wyników
eksperymentów wraz ze szczegółowymi wnioskami z nich płynącymi. Najcenniejsze
są, rzecz jasna, wnioski o charakterze uniwersalnym, które mogą być istotne
przy innych, podobnych zadaniach. Należy również omówić i wyjaśnić wszystkie
napotakane problemy (jeśli takie były). Każdy wniosek powinien mieć poparcie
we wcześniej przeprowadzonych eksperymentach (odwołania do konkretnych
wyników). Jest to jedna z najważniejszych sekcji tego sprawozdania, gdyż
prezentuje poziom zrozumienia badanego problemu.}
\section{Wnioski}
{\color{blue}W tej, przedostatniej, sekcji należy zamieścić podsumowanie
najważniejszych wniosków z sekcji poprzedniej. Najlepiej jest je po prostu
wypunktować. Znów, tak jak poprzednio, najistotniejsze są wnioski o
charakterze uniwersalnym.}


\begin{thebibliography}{0}
\end{thebibliography}
{\color{blue} 
Na końcu należy obowiązkowo podać cytowaną w sprawozdaniu
literaturę, z której grupa korzystała w trakcie prac nad zadaniem (przykład na
końcu szablonu)}
\end{document}
